#!.venv/bin/python

# expects to run from repo root

import json
import subprocess
import _jsonnet  # type: ignore
import requests
import re

from datetime import date

# Typing imports
from typing import TypeAlias

IMAGES_JSONNET_PATH = 'tanka/lib/images.libsonnet'
UPGRADE_PLAN_PATH = 'cluster-configuration/k3s-upgrade-plan.yaml'
JSONNETFILE_PATH = 'tanka/jsonnetfile.json'

HOTIO_RELEASE_REGEX = r'release-(?:\d+\.)+\d+'
TAG_REGEX = {
    'prowlarr': HOTIO_RELEASE_REGEX,
    'sonarr': HOTIO_RELEASE_REGEX,
    'radarr': HOTIO_RELEASE_REGEX,
    'qbittorrent': HOTIO_RELEASE_REGEX,
    'promtail': r'\d+\.\d+\.\d+',
    'tautulli': r'v\d+\.\d+\.\d+',
}

ImageDef: TypeAlias = dict[str, str]
TagData: TypeAlias = dict[str, str]
# The TagData TypeAlias is not strictly correct; there can be ints, but I'm not currently using them.
# Will expand this out if needed later


def main():
    check_for_image_updates()
    check_for_k3s_update()
    check_for_loki_update()


# Runs through the images lib and checks for updates for each image
# "followTag" is a tag that *should* follow a versioned release. Poll the tags of the image to find the version tag
# that matches its digest and use that.
# Once done, format the results back through jsonnetfmt and write to the lib file
def check_for_image_updates():
    images: dict[str, ImageDef] = json.loads(_jsonnet.evaluate_file(IMAGES_JSONNET_PATH))['images']
    for app, detail in images.items():
        if date.fromisoformat(detail.get('lastChecked', "1970-01-01")) == date.today():
            continue
        if detail['image'].startswith('ghcr'):
            continue
        image = detail['image']
        tag = detail['followTag']
        image_updates = get_image_update(app, image, tag)
        # Check if we found a unique image to use
        if len(image_updates) == 1:
            # Special case for Pihole, since it currently pre-pulls with a DaemonSet
            if app == 'pihole':
                images[app]['prepullImage'] = image_updates[0]
            else:
                images[app]['image'] = image_updates[0]
        else:
            print(f"Multiple tags found, skipping: {image} -> {image_updates}")
        # Commented this out because if I automate this, I don't want a new commit every day from the date change
        # Can just uncomment for rapid testing
        # images[app]['lastChecked'] = str(date.today())

    output = jsonnetfmt_dumps({'images': images})
    with open(IMAGES_JSONNET_PATH, "w") as f:
        f.write(output)
    print("Updated. Check diffs.")


# Check for update to a single image
def get_image_update(app: str, current_image: str, tag_to_check: str) -> list[str]:
    image_id, current_tag = current_image.rsplit(":", 1)
    image_tag_template = image_id + ":{}"
    if "/" in image_id:
        owner, image_name = image_id.rsplit("/", 1)
    else:
        # Official Docker Image. Can be queried with namespace/owner "library"
        owner = "library"
        image_name = image_id

    tags: dict[str, TagData] = {}

    # Query image repo in pages until we find the tag we want
    # Possible edge-case: tag we want is updated after another versioned tag, and that versioned tag is on
    # the next page. If I ever notice that happen, I'll add another page query here.
    page_size = 20  # Most images don't have a lot of tags
    page_num = 1
    page_max = 5  # ...but sometimes they publish a new tag for every fucking commit

    while tag_to_check not in tags.keys() and page_num <= page_max:
        tags_url = f'https://hub.docker.com/v2/namespaces/{owner}/repositories/{image_name}/tags' + \
                    f'?page_size={page_size}' + \
                    f'&page={page_num}'
        response = requests.get(tags_url)
        tags_list: list[TagData] = json.loads(response.text)['results']
        # Add the list into a dict keyed on the tag's name for ease of use later
        for tag_data in tags_list:
            if 'digest' in tag_data.keys():
                # idk if a missing digest means a tag is deprecated or something, but it can occur. Skip those.
                tags[tag_data['name']] = tag_data
        page_num += 1
    # Verify we got the tag
    if tag_to_check not in tags.keys():
        # As of this writing, this is purely for Promtail. They publish like 20 new tags per day.
        print(f"WARNING! {app} is stupid and has WAY too many tags. Couldn't find a tag to use, so skipping.")
        return [current_image]

    # Loop through the tags and find ones with digests matching the tag we want
    # The regex match is used to filter out duplicates for some images
    check_digest = tags[tag_to_check]['digest']
    potential_images = []
    for tag, data in tags.items():
        if data['digest'] == check_digest and \
           tag != tag_to_check and \
           tag != 'latest' and \
           re.match(TAG_REGEX.get(app, '.*'), tag):
            potential_images.append(image_tag_template.format(tag))

    # Did that work?
    if not len(potential_images):
        # If here, this app sucks and doesn't have a tag that follows versioned releases.
        # Resort to checking all tags that match the regex to see if any are newer than the current tag
        tag_detail_url = f'https://hub.docker.com/v2/namespaces/{owner}/repositories/{image_name}/tags/{current_tag}'
        tag_detail = json.loads(requests.get(tag_detail_url).text)
        newest_tag_date = tag_detail['tag_last_pushed']
        newest_tag = current_tag
        for tag, data in tags.items():
            if re.match(TAG_REGEX.get(app, '.*'), tag) and 'tag_last_pushed' in data.keys():
                date = data['tag_last_pushed']
                if date > newest_tag_date:
                    newest_tag_date = date
                    newest_tag = tag
        potential_images.append(image_tag_template.format(newest_tag))

    # Special check for Promtail, because their automation is dumb and will update "latest" to an older
    # version if it received a hotfix.
    # Version number should always go up. If it went down, return current image.
    if len(potential_images) == 1 and \
       potential_images[0] < current_image:
        return [current_image]

    return potential_images


# Checks for an update to k3s, and updates the upgrade plan file if so
def check_for_k3s_update():
    current_version = get_current_k3s_version()
    new_version = get_desired_k3s_version(current_version)
    if new_version != current_version:
        write_upgrade_plan_file(new_version)


# Queries k3s' GitHub repo tags and looks for a later release such that I'm 1 minor version behind latest
# (It's not uncommon I run into things that don't support bleeding edge k8s)
# This won't work if the "major" version changes, or if (more likely) they stop pretending it ever will and drop it
def get_desired_k3s_version(current_version: str) -> str:
    current_minor = int(current_version[3:5])
    latest = current_version
    latest_minor = current_minor
    tags = sort_tags(get_github_repo_tags('k3s-io/k3s'))
    for tag in tags:
        this_tag = tag['name']
        if "rc" not in this_tag:
            if this_tag > latest:
                latest = this_tag
                latest_minor = int(latest[3:5])
            else:
                minor_version = int(this_tag[3:5])  # assuming it starts with "v1.YY..."
                if latest_minor-1 == minor_version and latest_minor > current_minor:
                    # loop starts from latest tag. If we've found a tag that's one behind the latest, but newer than
                    # current, then we can stop. That's what we want.
                    return this_tag
    # If here, there's no later [desired] version; return current
    return current_version


# Gets current version of k3s from the upgrade plan file
def get_current_k3s_version() -> str:
    with open(UPGRADE_PLAN_PATH, "r") as file:
        for line in file:
            if 'version' in line and '+k3s' in line:
                return line.split(':')[1].strip()
    raise Exception("Couldn't find current version from upgrade plan file")


# Writes the provided version to the upgrade plan file
def write_upgrade_plan_file(new_version: str):
    new_file_contents = ""
    with open(UPGRADE_PLAN_PATH, "r+") as file:
        for line in file:
            if 'version' in line and '+k3s' in line:
                new_file_contents += f"  version: {new_version}\n"
            else:
                new_file_contents += line
        file.seek(0)
        file.write(new_file_contents)


# Checks for an update for Loki (and Promtail, in the same repo)
# Updates the jsonnetfile.json if one is found
def check_for_loki_update():
    current_version = get_current_loki_version()
    new_version = get_latest_loki_version()
    if new_version != current_version:
        write_new_loki_version(new_version)


# Gets current Loki version from jsonnetfile.json
def get_current_loki_version() -> str:
    with open(JSONNETFILE_PATH, "r") as file:
        jsonnetfile: dict = json.load(file)
        dependencies: list[dict] = jsonnetfile['dependencies']
        for item in dependencies:
            try:
                if 'loki.git' in item['source']['git']['remote']:
                    return item['version']
            except KeyError:
                # dependency isn't set up like expected for Loki; safe to skip
                continue
    print("WARNING - couldn't get current Loki version from jsonnetfile.json")
    return ''


# Gets the latest Loki release version
def get_latest_loki_version() -> str:
    tags = sort_tags(get_github_repo_tags('grafana/loki'))
    for tag in tags:
        # Loki uses "vX.Y.Z" naming semantics. It's almost certainly the first one, but still check for the 'v'
        this_tag = tag['name']
        if re.match(r"^v\d+\.\d+\.\d+$", this_tag):
            return this_tag
    raise Exception(f"Didn't find a valid tag for Loki =( List of tags returned from GitHub:\n{tags}")


# Writes the given Loki version to jsonnetfile.json
def write_new_loki_version(new_version: str):
    with open(JSONNETFILE_PATH, "r+") as file:
        jsonnetfile: dict[str, list[dict]] = json.load(file)  # typing not 100% true but good enough
        for item in jsonnetfile['dependencies']:
            try:
                if 'loki.git' in item['source']['git']['remote']:
                    item['version'] = new_version
            except KeyError:
                # dependency isn't set up like expected for Loki; safe to skip
                continue
        file.seek(0)
        json.dump(jsonnetfile, file, indent=2)


# Helper for getting tags from GitHub repo
def get_github_repo_tags(repo: str) -> list[TagData]:
    url = f'https://api.github.com/repos/{repo}/tags'
    response = requests.get(url, headers={'Accept': 'application/vnd.github+json'})
    tags_list: list[TagData] = json.loads(response.text)
    return tags_list


# Wrapper for running the output of json.dumps through jsonnetfmt
def jsonnetfmt_dumps(input: dict) -> str:
    json_str = json.dumps(input, indent=2, sort_keys=True)
    return subprocess.run(["jsonnetfmt", "-"], input=json_str, text=True, capture_output=True).stdout


# Wrapper for sorting tags by name, defaulted to reverse order
# (since usually its version numbers, where larger = newer)
def sort_tags(sort_me: list[TagData], reverse=True) -> list[TagData]:
    return sorted(sort_me, key=lambda x: x['name'], reverse=reverse)


if __name__ == "__main__":
    main()
